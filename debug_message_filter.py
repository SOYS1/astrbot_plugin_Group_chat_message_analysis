#!/usr/bin/env python3
"""
è°ƒè¯•æ¶ˆæ¯è¿‡æ»¤é—®é¢˜ - æ£€æŸ¥ä¸ºä»€ä¹ˆè·å–åˆ°æ¶ˆæ¯ä½†æ²¡æœ‰è¯†åˆ«ä¸ºç›¸å…³
"""

import sys
import os
import json
from datetime import datetime, timedelta

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src', 'core'))

class DebugMessageFilter:
    """è°ƒè¯•æ¶ˆæ¯è¿‡æ»¤å™¨"""
    
    def __init__(self):
        self.group_id = "753075035"
        self.keyword = "æŠ¥é”™"
        
    def _expand_keyword_variants(self, keyword: str):
        """å…³é”®è¯æ‰©å±•"""
        keyword = keyword.lower().strip()
        variants = [keyword]
        
        # æ‰©å±•åˆ—è¡¨
        extensions = [
            "é”™è¯¯", "å¼‚å¸¸", "bug", "æ•…éšœ", "é—®é¢˜", "error", "exception", "å´©æºƒ", "å‡ºé”™",
            "æ€ä¹ˆåŠ", "æ€ä¹ˆè§£å†³", "æ±‚åŠ©", "å¸®å¸®æˆ‘", "æ•‘å‘½", "æ€¥", "åœ¨çº¿ç­‰",
            "åäº†", "ä¸è¡Œäº†", "å‡ºé—®é¢˜äº†", "å´©äº†", "æŒ‚äº†", "æ­»æœºäº†",
            "æ‰“ä¸å¼€", "æ— æ³•å¯åŠ¨", "è¿è¡Œä¸äº†", "é—ªé€€äº†", "æ— å“åº”"
        ]
        
        variants.extend(extensions)
        return list(set(variants))
    
    def _is_semantically_related(self, text: str, keyword_variants: list) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸å…³é”®è¯ç›¸å…³"""
        text = text.lower().strip()
        if not text:
            return False
        
        print(f"\nğŸ” æ£€æŸ¥æ¶ˆæ¯: '{text}'")
        print(f"å…³é”®è¯å˜ä½“: {keyword_variants}")
        
        # 1. ç›´æ¥åŒ…å«æ£€æŸ¥
        for variant in keyword_variants:
            if variant in text:
                print(f"âœ… ç›´æ¥åŒ¹é…: '{variant}' åœ¨æ¶ˆæ¯ä¸­")
                return True
        
        # 2. å­—ç¬¦ç›¸ä¼¼åº¦æ£€æŸ¥
        import difflib
        for variant in keyword_variants:
            similarity = difflib.SequenceMatcher(None, text, variant).ratio()
            if similarity >= 0.3:  # é™ä½é˜ˆå€¼
                print(f"âš ï¸ å­—ç¬¦ç›¸ä¼¼åº¦: '{variant}' (ç›¸ä¼¼åº¦: {similarity:.2f})")
                if similarity >= 0.5:
                    return True
        
        # 3. åˆ†è¯æ£€æŸ¥
        def simple_tokenize(text: str):
            import re
            return re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z0-9]+', text.lower())
        
        text_tokens = simple_tokenize(text)
        keyword_tokens = []
        for variant in keyword_variants:
            keyword_tokens.extend(simple_tokenize(variant))
        
        for kw_token in keyword_tokens:
            for text_token in text_tokens:
                if kw_token in text_token or text_token in kw_token:
                    if len(kw_token) >= 2:
                        print(f"âœ… åˆ†è¯åŒ¹é…: '{kw_token}' ~ '{text_token}'")
                        return True
        
        print("âŒ æ— åŒ¹é…")
        return False
    
    def debug_filtering(self):
        """è°ƒè¯•æ¶ˆæ¯è¿‡æ»¤"""
        print("ğŸ¯ è°ƒè¯•æ¶ˆæ¯è¿‡æ»¤é—®é¢˜")
        print("=" * 50)
        
        # æ¨¡æ‹Ÿç¾¤æ¶ˆæ¯
        mock_messages = [
            "æœåŠ¡å™¨çªç„¶å´©äº†ï¼Œæ€ä¹ˆåŠï¼Ÿ",
            "ç¨‹åºæŠ¥é”™äº†ï¼Œæœ‰äººèƒ½å¸®å¿™å—ï¼Ÿ",
            "è¿™ä¸ªbugæ€ä¹ˆè§£å†³ï¼Ÿ",
            "ç³»ç»Ÿå‡ºé—®é¢˜äº†ï¼Œåœ¨çº¿ç­‰ï¼",
            "æŠ¥é”™ä¿¡æ¯çœ‹ä¸æ‡‚ï¼Œæ±‚åŠ©",
            "è½¯ä»¶å¼‚å¸¸é€€å‡ºï¼Œæ€¥ï¼",
            "è¿è¡Œä¸äº†ï¼Œå¸®å¸®å¿™",
            "å‡ºç°é”™è¯¯ä»£ç ï¼Œå¤§ä½¬æ•‘æ€¥",
            "æœåŠ¡å™¨æŒ‚äº†ï¼Œæ•‘å‘½å•Šï¼",
            "ç¨‹åºæ‰“ä¸å¼€ï¼Œå´©äº†",
            "è¿™å•¥æƒ…å†µå•Šï¼Œä¸€ç›´æŠ¥é”™",
            "æœ‰äººé‡åˆ°è¿‡è¿™ä¸ªé—®é¢˜å—ï¼Ÿ",
            "æ±‚å¤§ä½¬å¸®å¿™çœ‹çœ‹è¿™ä¸ªé”™è¯¯",
            "ç¨‹åºè¿è¡Œä¸äº†ï¼Œæ€¥æ­»äº†",
            "ç³»ç»Ÿå´©äº†ï¼Œæ€ä¹ˆä¿®å¤ï¼Ÿ"
        ]
        
        keyword_variants = self._expand_keyword_variants("æŠ¥é”™")
        print(f"å…³é”®è¯æ‰©å±•ç»“æœ: {keyword_variants}")
        
        matched_count = 0
        for i, msg in enumerate(mock_messages, 1):
            print(f"\n--- æ¶ˆæ¯ {i} ---")
            is_related = self._is_semantically_related(msg, keyword_variants)
            if is_related:
                matched_count += 1
                print(f"ğŸ¯ åŒ¹é…æˆåŠŸ: {msg}")
        
        print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"æ€»æ¶ˆæ¯æ•°: {len(mock_messages)}")
        print(f"åŒ¹é…æˆåŠŸ: {matched_count}")
        print(f"åŒ¹é…ç‡: {matched_count/len(mock_messages)*100:.1f}%")
        
        # æ£€æŸ¥å¯èƒ½çš„é—®é¢˜
        print(f"\nğŸ”§ å¯èƒ½çš„é—®é¢˜:")
        print("1. å…³é”®è¯æ‰©å±•æ˜¯å¦å®Œæ•´ï¼Ÿ")
        print("2. ç›¸ä¼¼åº¦é˜ˆå€¼æ˜¯å¦è¿‡é«˜ï¼Ÿ")
        print("3. æ¶ˆæ¯å†…å®¹æ˜¯å¦è¢«é¢„å¤„ç†ï¼Ÿ")
        print("4. æ—¶é—´èŒƒå›´æ˜¯å¦åˆé€‚ï¼Ÿ")
        
        return matched_count > 0

if __name__ == "__main__":
    debugger = DebugMessageFilter()
    debugger.debug_filtering()